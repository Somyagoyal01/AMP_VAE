# AMP_VAE: Generative Modeling for Antimicrobial Peptide Design

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Overview

This project implements a Variational Autoencoder (VAE) in TensorFlow/Keras to generate novel antimicrobial peptide (AMP) sequences. AMPs are a promising class of drugs for combating antibiotic-resistant bacteria. The VAE is trained on curated peptide sequences from the DBAASP database to learn underlying patterns and generate new sequences with potential antimicrobial activity.

## Key Features

*   Implements a Variational Autoencoder (VAE) architecture using TensorFlow/Keras.
*   Trained on over 18,000 curated AMP sequences from the DBAASP database.
*   Generates novel peptide sequences with a customizable latent space dimension (32).
*   Provides code for data preprocessing, model training, and sequence generation.

## Data

The model was trained on over 18,000 peptide sequences sourced from the DBAASP (Database of Antimicrobial Activity and Structure of Peptides) database. The data was preprocessed to remove duplicates, filter sequences by length (5-50 amino acids), and encode amino acids into numerical representations.

## Code

The following files make up the project:

*   `data_preparation.py`: Loads, cleans, and preprocesses the peptide data from the DBAASP database.
*   `vae_model.py`: Defines the Variational Autoencoder (VAE) architecture using TensorFlow/Keras. Includes the encoder, decoder, and a custom sampling layer.
*   `train.py`: Trains the VAE model on the preprocessed peptide data.
*   `generate.py`: Generates novel peptide sequences using the trained VAE model.

## Installation and Usage

1.  **Clone the repository:**

    ```bash
    git clone https://github.com/Somyagoyal01/AMP_VAE.git
    cd AMP_VAE
    ```

2.  **Create a virtual environment (recommended):**

    ```bash
    python -m venv .venv
    source .venv/bin/activate  # On Linux/macOS
    .venv\Scripts\activate  # On Windows
    ```

3.  **Install dependencies:**

    ```bash
    pip install numpy tensorflow pandas scikit learn
    
4.  **Download the DBAASP dataset:**

    *   Go to the DBAASP website ([https://dbaasp.org/](https://dbaasp.org/)).
    *   Use the search/export function to download peptide sequences and metadata in CSV format.
    *   You will need a DBAASP Account

5.  **Configure the data path:** Update the `data_path` variable in `data_preparation.py`, `train.py`, and `generate.py` to point to the location of your downloaded data file (e.g., `"data/peptides.csv"`).

6.  **Run the data preparation script:**

    ```bash
    python data_preparation.py
    ```
    Confirm this output:
    Shape of encoded training data: (xxxx, 50) (Where xxxx is the number of rows in my table. This represents a split to ensure accurate training metrics)
    Shape of encoded testing data: (xxxx, 50) (Where xxxx is the number of rows in my testing data table. This represents a split to ensure accurate testing metrics)

7.  **Run the training script:**

    ```bash
    python train.py
    ```
    
    
 We can also verify the success of the operation using the "models have been saved" or the loss shown during data training
 
8.  **Run the generation script:**

    ```bash
    python generate.py
    ```
## Project Structure
AMP_VAE/ (My Project Root)
data/
peptides.csv (My downloaded DBAASP data)
vae_model.py
data_preparation.py
train.py
generate.py
README.md
LICENSE
.gitignore

## Example Generated Peptides

Here are some example peptide sequences generated by the trained VAE:

1)KLLKKKKKLLL
2)GGKKKKKKKKKKKKKKKKKKKKKKKKK
3)KKKKKKKKKKKKKK
4)GKKKKKKKKKKKKKKKKK
5)GLKKKKKKKKKKKKKKKKK
6)LWWWWW
7)GGGKKKKKKKKKKKKKKKGGGGGGGGGGGKKKKKKK
8)KLLLKKKKLLL
9)KLLKKKKKKKK
10)GKKKKKKKKKKKKKKKKKK etc


## Implementation Details

The VAE architecture consists of:

*   **Encoder:** An embedding layer, followed by an LSTM layer, and two dense layers to output the mean (z_mean) and log variance (z_log_var) of the latent distribution. A custom sampling layer is used for reparameterization.
*   **Decoder:** A dense layer, a repeat vector layer, an LSTM layer, and a time-distributed dense layer with a softmax activation to output the reconstructed peptide sequence.
## Troubleshooting

During development, the following issues were faced, some areas to consider include:
* Incompatible versions - use tensorflow, and in this case keras 2.13+
* Serialized training issues with custom layers.

## Acknowledgments
Shout-out to many online sources and stack overflow for helping with the creation of this particular project!

## Code Author

Somya Goyal - [https://github.com/Somyagoyal01](https://github.com/Somyagoyal01)

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contact

Somya Goyal - jkgsomya12@gmail.com
